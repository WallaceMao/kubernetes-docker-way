# 阿里云kubernetes容器服务架构及容器部署步骤（下）

本着实用的原则，本教程没有过深的理论解释，通过示例来帮助大家快速熟悉docker以及kubernetes操作

## kubernetes

### 基础配置操作

以下示例在[阿里云kubernetes容器服务架构及容器部署步骤（上）](2.md)的基础上，service/deploy/pod相关的内容不再具体解释。

首先在集群的管理服务器上新建`/root/k8s-tutorial`目录，后面的示例中的配置文件，我们均会在该目录下存储。

#### 给nginx服务限定资源配额

新增`/root/k8s-tutorial/1-resources`目录，用来存储本示例中的文件。

- （1）新增service配置文件，并创建service

新建`service-nginx.yaml`文件，并写入如下内容：

```
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  labels:
    app: test-nginx
spec:
  selector:
    app: test-nginx
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080
  type: NodePort
```

运行命令`kubectl create -f service-nginx.yaml`，创建service

- （2）新增deploy配置文件，并创建deploy

新建`deploy-nginx.yaml`文件，并写入如下内容：
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: test-nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: test-nginx
  template:
    metadata:
      labels:
        app: test-nginx
    spec:
      containers:
      - name: test-nginx
        image: registry-vpc.cn-beijing.aliyuncs.com/rsq-public/nginx:1.13.12
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"

```

说明：
- spec.template.spec.containers[0].resources.requests：初始请求所需的资源
- spec.template.spec.containers[0].resources.requests.memory：初始请求所需的内存资源
- spec.template.spec.containers[0].resources.requests.cpu：初始请求所需的cpu资源
- spec.template.spec.containers[0].resources.limits：可以使用的资源的限制
- spec.template.spec.containers[0].resources.limits.memory：可以使用的内存资源的限制
- spec.template.spec.containers[0].resources.limits.cpu：可以使用的cpu资源的限制

内存的单位`Mi`、`Gi`分别表示1M和1G。对于一般的java程序，至少需要`2Gi`的空间
cpu的单位为`m`，一核cpu为1000m，以此类推

运行命令`kubectl create -f deploy-nginx.yaml`，创建deploy

- （3）查看资源限额

运行命令`kubectl get pod -o wide`，获得`pod`所在的`node`的nodeName

运行命令`kubectl describe node [nodeName]`查看pod的资源占用

- （4）删除nginx服务

`kubectl delete -f service-nginx.yaml`

`kubectl delete -f deploy-nginx.yaml`


#### 给nginx指定liveness和readiness探针

新增`/root/k8s-tutorial/2-probe`目录，用来存储本示例中的文件。

- （1）新增service配置文件，并创建service

新建`service-nginx.yaml`文件，并写入如下内容：

```
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  labels:
    app: test-nginx
spec:
  selector:
    app: test-nginx
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080
  type: NodePort
```

运行命令`kubectl create -f service-nginx.yaml`，创建service

- （2）新增deploy配置文件，并创建deploy

新建`deploy-nginx.yaml`文件，并写入如下内容：
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: test-nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: test-nginx
  template:
    metadata:
      labels:
        app: test-nginx
    spec:
      containers:
      - name: test-nginx
        image: registry-vpc.cn-beijing.aliyuncs.com/rsq-public/nginx:1.13.12
        ports:
        - containerPort: 80
        livenessProbe:
          httpGet:
            path: /task/css/page.css
            port: 8080
          initialDelaySeconds: 360
          timeoutSeconds: 5
          periodSeconds: 5
        readinessProbe:
          httpGet:
            path: /task/css/page.css
            port: 8080
          initialDelaySeconds: 120
          timeoutSeconds: 5
          periodSeconds: 5
```

说明：
- spec.template.spec.containers[0].ivenessProbe：存活探针，用来检测应用是否存活，如果检测失败，pod将会被自动重启
- spec.template.spec.containers[0].ivenessProbe.httpGet.path：http请求检测的路径
- spec.template.spec.containers[0].ivenessProbe.httpGet.port：http请求检测的端口
- spec.template.spec.containers[0].ivenessProbe.initialDelaySeconds：项目刚启动时，在多少秒内不进行存活检测
- spec.template.spec.containers[0].ivenessProbe.timeoutSeconds：超时时间，应用响应时间超过这个时间将会被判定检测失败
- spec.template.spec.containers[0].ivenessProbe.periodSeconds：检测周期，检测开始后，每隔多长时间进行一次检测
- spec.template.spec.containers[0].eadinessProbe：启动检测，用来检测应用是否可以对外提供服务，如果检测失败，应用将不会被分配流量
- spec.template.spec.containers[0].eadinessProbe.httpGet.path：http请求检测的路径
- spec.template.spec.containers[0].eadinessProbe.httpGet.port：http请求检测的端口
- spec.template.spec.containers[0].eadinessProbe.initialDelaySeconds：项目启动（或被重启）时，在多少秒内不进行检测
- spec.template.spec.containers[0].eadinessProbe.timeoutSeconds：超时时间，应用响应时间超过这个时间将会被判定检测失败
- spec.template.spec.containers[0].eadinessProbe.periodSeconds：检测周期，检测开始后，每隔多长时间进行一次检测

特别注意的有以下几点：
- 存活检测的`initialDelaySeconds`：这个值应该视具体应用的启动时间而定，应该比应用预估的最长启动时间再大一些。对于java应用，假设启动时间在180s（3分钟左右），而存活检测的`initialDelaySeconds`如果设置为60s，那么60s内探针无法检测成功，会强制应用重启。这样导致的结果是应用反复重启而不能正常启动！
- 存活检测的`periodSeconds`：这个值不宜过长，过长会导致应用已经宕了，但是迟迟没有重启恢复
- 启动检测的`initialDelaySeconds`：这个值应该略小于应用的预估启动时间，保证一旦项目启动成功，可以及时检测到。

运行命令`kubectl create -f deploy-nginx.yaml`，创建deploy

启动时，可以观察鉴活探针和启动探针的具体表现

- （3）删除nginx服务

`kubectl delete -f service-nginx.yaml`

`kubectl delete -f deploy-nginx.yaml`

#### 使用persistent volume

persistent volume（简写为pv）是对存储介质的抽象。存储介质包括：硬盘、NAS盘、NFS存储等。对于演示类的项目，可以使用本地磁盘作为pv，而在线上环境中，推荐使用云服务商的NAS、OSS等分布式存储作为pv

以下以阿里云的NAS为例，建立pv

persistent volume claim（简写为pvc）是对镜像与pv的连接的抽象。

新增`/root/k8s-tutorial/3-pv`目录，用来存储本示例中的文件。

- （1）新增service配置文件，并创建service

新建`service-nginx.yaml`文件，并写入如下内容：

```
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  labels:
    app: test-nginx
spec:
  selector:
    app: test-nginx
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080
  type: NodePort
```

运行命令`kubectl create -f service-nginx.yaml`，创建service

- （2）阿里云NAS中新增html文件

创建阿里云NAS服务，并挂载到ECS中，然后在其中创建`/test-nginx-project/hello.html`文件：

```
<!DOCTYPE html>
<html>
<head>
	<title>hello</title>
</head>
<body>
  hello world
</body>
</html>
```

- （3）新增pv配置文件，并创建pv

新建`pv-nginx.yaml`文件，并写入如下内容：
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: test-nginx-project-pv
spec:
  capacity:
    storage: 1M
  storageClassName: test-nginx-project-pv
  accessModes:
    - ReadOnlyMany
  flexVolume:
    driver: "alicloud/nas"
    options:
      server: "xxxxxxxxx-jog17.cn-beijing.nas.aliyuncs.com"
      path: "/test-nginx-project"
      vers: "4.0"
      mode: "755"

```

说明：
- spec.capacity.storage：存储的空间大小
- spec.storageClassName：存储的类，这个类会在pvc的定义中用到
- spec.accessModes：读取的方式
- spec.flexVolume.driver：pv驱动
- spec.flexVolume.options.server：阿里云nas提供的nas server的地址
- spec.flexVolume.options.path：nas中的映射的目录
- spec.flexVolume.options.vers：阿里云nas的版本
- spec.flexVolume.options.mode：阿里云nas中文件的mode

运行命令`kubectl create -f pv-nginx.yaml`，创建pv

运行命令`kubectl get pv`，查看创建的pv

- （4）新增pvc配置文件，并创建pvc

新建`pvc-nginx.yaml`文件，并写入如下内容：

```
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-nginx-project-pvc
spec:
  accessModes:
    - ReadOnlyMany
  storageClassName: test-nginx-project-pv
  resources:
    requests:
      storage: 1M

```

说明：
- spec.accessModes：权限不能大于pv
- spec.storageClassName：与pv对应的storageClass
- spec.resources.requests.storage：请求的空间大小

运行命令`kubectl create -f pvc-nginx.yaml`，创建pvc

运行命令`kubectl get pvc`，查看创建的pvc

- （5）新增deploy配置文件，并创建deploy

新建`deploy-nginx.yaml`文件，并写入如下内容：
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: test-nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: test-nginx
  template:
    metadata:
      labels:
        app: test-nginx
    spec:
      containers:
      - name: test-nginx
        image: registry-vpc.cn-beijing.aliyuncs.com/rsq-public/nginx:1.13.12
        ports:
        - containerPort: 80
        volumeMounts:
        - name: base-project
          mountPath: /usr/share/nginx/html
      volumes:
      - name: base-project
        persistentVolumeClaim:
          claimName: test-nginx-project-pvc
```

说明：
- spec.template.spec.volumes[0].name：引入的volume的名字
- spec.template.spec.volumes[0].persistentVolumeClaim.claimName：引入的volume对应的pvc，需要与之前创建的pvc的name对应
- spec.template.spec.containers[0].volumeMounts[0].name：与spec.template.spec.volumes[0].name对应
- spec.template.spec.containers[0].volumeMounts[0].mountPath：映射到容器中的路径

需要注意的是映射到容器中的指定路径后，路径如果已存在，那么路径中的文件会全部丢失，只会加载pv中的文件

- （6）测试

访问`localhost:30080/hello.html`可以成功返回页面

- （7）删除nginx服务

`kubectl delete -f service-nginx.yaml`

`kubectl delete -f deploy-nginx.yaml`

`kubectl delete -f pvc-nginx.yaml`

`kubectl delete -f pv-nginx.yaml`

#### 使用configmap

新增`/root/k8s-tutorial/4-configmap`目录，用来存储本示例中的文件。

- （1）新增service配置文件，并创建service

新建`service-nginx.yaml`文件，并写入如下内容：

```
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  labels:
    app: test-nginx
spec:
  selector:
    app: test-nginx
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080
  type: NodePort
```

运行命令`kubectl create -f service-nginx.yaml`，创建service

- （2）新增configmap配置文件，并创建configmap

新建`configmap-nginx.yaml`文件，并写入如下内容：

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-nginx-config
data:
  default.conf: |
    server {
        listen 80 default_server;
        server_name default_server;

        location / {
            proxy_pass https://www.rishiqing.com
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   /usr/share/nginx/html;
        }
    }

```

说明：
- data：key值为挂载到容器内的文件名。value值为文件的内容。注意value值中关于yaml文件格式的处理。

运行命令`kubectl create -f configmap-nginx.yaml`，创建configmap

运行命令`kubectl get configmap`，查看configmap

- （3）新增deploy配置文件，并创建deploy

新建`deploy-nginx.yaml`文件，并写入如下内容：
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: test-nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: test-nginx
  template:
    metadata:
      labels:
        app: test-nginx
    spec:
      containers:
      - name: test-nginx
        image: registry-vpc.cn-beijing.aliyuncs.com/rsq-public/nginx:1.13.12
        ports:
        - containerPort: 80
        volumeMounts:
        - name: base-config
          mountPath: /etc/nginx/conf.d
      volumes:
      - name: base-config
        configMap:
          name: test-nginx-config
```

说明：
- spec.template.spec.volumes[0].name：引入的volume的名字
- spec.template.spec.volumes[0].configMap.name：引入的configmap的名称
- spec.template.spec.containers[0].volumeMounts[0].name：与spec.template.spec.volumes[0].name对应
- spec.template.spec.containers[0].volumeMounts[0].mountPath：映射到容器中的路径

运行命令`kubectl create -f deploy-nginx.yaml`，创建deploy

- （4）测试

访问`localhost:30080`可以直接跳转到[日事清](https://www.rishiqing.com)

- （4）删除nginx服务

`kubectl delete -f service-nginx.yaml`

`kubectl delete -f deploy-nginx.yaml`

`kubectl delete -f configmap-nginx.yaml`

#### secret基本操作

相对于configmap，secret会对其中的数据进行base64 hash存储。其用法与configmap类似，也可以映射通过volumeMounts挂载到容器中

- （1）创建docker镜像仓库的密钥

`kubectl create secret docker-registry [secretName] --docker-server=registry-vpc.cn-beijing.aliyuncs.com --docker-username=aaaa@xxx.com --docker-password=111111 --docker-email=hello@123.com`

- docker-registry：表示secret的类型，可以用来执行docker login
- secretName：secret的名称
- --docker-server：docker仓库的地址
- --docker-username：登录用户名
- --docker-password：登录密码
- --docker-email：邮箱

- （2）在拉取docker镜像时使用secret

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: test-nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: test-nginx
  template:
    metadata:
      labels:
        app: test-nginx
    spec:
      containers:
      - name: test-nginx
        image: registry-vpc.cn-beijing.aliyuncs.com/rsq-public/nginx:1.13.12
        ports:
        - containerPort: 80
      imagePullSecrets:
      - name: ali-docker
```

说明：
- spec.template.spec.imagePullSecrets.name：secret的名称

### 集群操作——deploy集群更新、伸缩与回滚

k8s集群中可以方便地进行更新、伸缩和回滚操作

#### 查看容器运行日志

滚动显示一个pod的日志：

`kubectl log -f [podName]`

滚动显示一个pod的日志，且只显示后100行。由于输出的日志能比较多，因此`--tail`参数会很常用

`kubectl log -f --tail=100 [podName]`

当pod重启后，`-p`参数可以查看前一个`pod`的日志，从而查找`pod`重启的原因

`kubectl log -p --tail=100 [podName]`

#### 集群更新

更新k8s中镜像版本：

`kubectl set image deployment/[deployName] [containerName]=[imagePath]`

说明：
- deployName：deploy的名称
- containerName：deploy的pod中的container的名称
- imagePath：待更新的镜像的路径

- （1）新建deploy

新建`deploy-nginx.yaml`文件，并写入如下内容：

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: test-nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: test-nginx
  template:
    metadata:
      labels:
        app: test-nginx
    spec:
      containers:
      - name: test-nginx
        image: registry-vpc.cn-beijing.aliyuncs.com/rsq-public/nginx:1.13.12
        ports:
        - containerPort: 80
```

- （2）更新image版本

`kubectl set image deployment nginx-deploy test-nginx=registry-vpc.cn-beijing.aliyuncs.com/rsq-public/nginx:1.14`

可以将nginx更新成`1.14`版本

通过命令`kubectl describe deployment nginx-deploy`可以查看是否更新到最新版本

#### 集群回滚

- （1）查看历史版本
运行命令

`kubectl rollout history deployment nginx-deploy`

查看历史版本。如果要查看某个历史版本的详细信息，可以运行命令：

`kubectl rollout history deployment nginx-deploy --revision=[revisionNumber]`

其中[revisionNumber]为之前命令中查看到的版本号。在详细信息中可以看到具体的镜像版本

- （2）将deploy回滚到之前的某个版本

`kubectl rollout undo deployment nginx-deploy --to-revision=[revisionNumber]`

其中[revisionNumber]为需要回滚到的版本

#### 集群伸缩

将副本数量伸缩成1个：

`kubectl scale deployment nginx-deploy --replicas=1`

### 将应用发布到kubernetes中

由于kubernetes集群中的应用是无状态的，因此如果要将一个应用发布到kubernetes中，首先需要考虑以下几个方面：
- 配置文件的存储方式。configmap/secret/pv
- 日志文件的输出方式。pv/console/日志服务
- 其他状态文件的存储，例如上传的文件等。pv/公共存储
